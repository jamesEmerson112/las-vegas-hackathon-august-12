# -*- coding: utf-8 -*-
"""Hack Night Vegas - August 12 - Comet -> Weaviate

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VaRD3gzcI6gsaXIXNiIlDwNyKghzPYMf

# Hack Night in Vegas - August 12, 2025

![Image](https://images.lumacdn.com/cdn-cgi/image/format=auto,fit=cover,dpr=2,background=white,quality=75,width=300,height=300/event-covers/g5/67cd5e0c-c67c-4da2-86f2-57d9c5790089.png)

Today, we're going to explore observability over our RAG Applications. [Weaviate](https://weaviate.io/) provides the retrieval, [FriendliAI](https://friendli.ai/) provides the inference layer, and [Comet Opik](https://comet.com/opik) is our observability layer.

This simple example will get you started with using Opik, Weaviate, and Friendli Serverless Endpoints to build a RAG system.

To use this notebook successfully, you'll need an account with Comet, Friendli and Weaviate.


**Note:** A Weaviate cluster is already set up, so you technically don't need to create a new cluster, and you can just READ off an existing cluster. If you want to learn more about how this cluster was set up, check out the `weaviate-embeddings-and-friendliai` dierctory in [this repository](https://github.com/weaviate/BookRecs/tree/main/data-pipeline).

You can create free accounts on all platforms.

# Set up your Environment with Comet Opik

[Comet](https://www.comet.com/) provides a hosted version of the Opik platform, simply [create a free account](https://www.comet.com/site/products/opik/) and grab you API Key from the UI.

First, we need pip install the opik and openai libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U opik openai --quiet

"""Now, we'll configure Opik and FriendliAI with our respective API keys."""

import opik

opik.configure(use_local=False)

"""Traces will now be automatically logged to the Opik UI where you can inspect the inputs, outputs, and configure evaluation metrics. After you run this cell, follow the link to the Comet UI to see you traces.

# Set up Weaviate Client

Weaviate is a vector database which supports billion scale vector search with sub 50ms query times. We'll use Weaviate to query for books in this example.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U weaviate-client --quiet

import os
import weaviate
from weaviate.classes.init import Auth
from weaviate.classes.init import AdditionalConfig, Timeout


WEAVIATE_CLUSTER_URL = os.getenv('WEAVIATE_CLUSTER_URL') or 'https://4oreows2qroxgn0tjgj2uq.c0.us-west3.gcp.weaviate.cloud'
WEAVIATE_API_KEY = os.getenv('WEAVIATE_API_KEY') or 'aVcyNUdKT2d3WHMxcHFzYl9CL3haUXVkajhzSWtzRmFFamRWa0dOSjZEVGR1SVBWNTYzT21iSkVVeWJVPV92MjAw'

weaviate_client = weaviate.connect_to_weaviate_cloud(
    cluster_url=WEAVIATE_CLUSTER_URL,
    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),
    # Remove OpenAI header since we're using Ollama
)

print(weaviate_client.is_connected())

# Let's first check what collections are available
print("Available collections:")
try:
    collections = weaviate_client.collections.list_all()
    for collection_name in collections:
        print(f"  - {collection_name}")

    # Check if BookRecs collection exists, if not create it
    if "BookRecs" not in collections:
        print("üìö Creating BookRecs collection...")

        # Create the collection with proper schema
        from weaviate.classes.config import Configure, Property, DataType

        try:
            weaviate_client.collections.create(
                name="BookRecs",
                properties=[
                    Property(name="title", data_type=DataType.TEXT),
                    Property(name="author", data_type=DataType.TEXT),
                    Property(name="genre", data_type=DataType.TEXT),
                    Property(name="description", data_type=DataType.TEXT),
                    Property(name="year", data_type=DataType.INT),
                ],
                # Use text2vec-transformers which is more commonly available
                vectorizer_config=Configure.Vectorizer.text2vec_transformers(),
            )
        except Exception as vectorizer_error:
            print(f"Failed with text2vec-transformers, trying without vectorizer: {vectorizer_error}")
            # Fallback: create without vectorizer
            weaviate_client.collections.create(
                name="BookRecs",
                properties=[
                    Property(name="title", data_type=DataType.TEXT),
                    Property(name="author", data_type=DataType.TEXT),
                    Property(name="genre", data_type=DataType.TEXT),
                    Property(name="description", data_type=DataType.TEXT),
                    Property(name="year", data_type=DataType.INT),
                ],
            )

        # Add sample book data
        book_data = [
            {
                "title": "Dune",
                "author": "Frank Herbert",
                "genre": "Science Fiction",
                "description": "A science fiction epic about politics, religion, and ecology on the desert planet Arrakis.",
                "year": 1965
            },
            {
                "title": "The Hobbit",
                "author": "J.R.R. Tolkien",
                "genre": "Fantasy",
                "description": "A fantasy adventure about Bilbo Baggins and his journey with dwarves to reclaim their homeland.",
                "year": 1937
            },
            {
                "title": "1984",
                "author": "George Orwell",
                "genre": "Dystopian Fiction",
                "description": "A dystopian novel about totalitarianism and surveillance in a future society.",
                "year": 1949
            },
            {
                "title": "Foundation",
                "author": "Isaac Asimov",
                "genre": "Science Fiction",
                "description": "A space opera about psychohistory and the fall and rise of galactic civilizations.",
                "year": 1951
            },
            {
                "title": "The Lord of the Rings",
                "author": "J.R.R. Tolkien",
                "genre": "Fantasy",
                "description": "An epic fantasy about the quest to destroy the One Ring and defeat the Dark Lord Sauron.",
                "year": 1954
            },
            {
                "title": "Neuromancer",
                "author": "William Gibson",
                "genre": "Cyberpunk",
                "description": "A cyberpunk novel about hackers, artificial intelligence, and virtual reality.",
                "year": 1984
            }
        ]

        book_collection = weaviate_client.collections.get("BookRecs")

        print("üìñ Adding sample books to collection...")
        with book_collection.batch.dynamic() as batch:
            for book in book_data:
                batch.add_object(properties=book)

        print("‚úÖ BookRecs collection created and populated!")
    else:
        print("‚úÖ BookRecs collection found!")

    book_collection = weaviate_client.collections.get(name="BookRecs")

except Exception as e:
    print(f"Error setting up collections: {e}")
    exit(1)

"""# Write a RAG app with Weaviate and Opik Traces

Next, we will build a very simple LLM reasoning application and log the trace data to Opik where we can apply additional evaluation metrics and debug the LLM response.

We will use OpenAI as our inference provider to get fast,
We will use Opik to collect traces to inspect the inputs and outputs of the reasoning tasks, and to create evaluation metrics for hallicinations and other common or custom issues you want to detect.

Opik integrates with OpenAI to provide a simple way to log traces for all OpenAI LLM calls. This works for all OpenAI models, including if you are using the streaming API.
"""

from opik.integrations.openai import track_openai
from openai import OpenAI

os.environ["OPIK_PROJECT_NAME"] = "rag-project" #name your project. This will appear as the project name in the Opik UI

# Use Ollama instead of OpenAI
ollama_client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # Ollama doesn't require real API key
)

@opik.track
def call_llm(client, messages):
    response = ollama_client.chat.completions.create(
      model="llama3.2",  # Use Ollama model
      messages=messages
    )
    return response

# Test with a sample query instead of input
user_query = "science fiction books about space"
print(f"üîç Searching for: {user_query}")

try:
    # Try vector search first
    response = book_collection.query.near_text(
            query=user_query,
            limit=3
        )
    print("üìö Found books using vector search:")
    for book in response.objects:
        print(f"  - {book.properties['title']}")
except Exception as e:
    print(f"Vector search failed: {e}")
    # Fallback to simple text search
    try:
        response = book_collection.query.bm25(
            query=user_query,
            limit=3
        )
        print("üìö Found books using BM25 search:")
        for book in response.objects:
            print(f"  - {book.properties['title']}")
    except Exception as e2:
        print(f"BM25 search also failed: {e2}")
        # Final fallback: get all books
        response = book_collection.query.fetch_objects(limit=3)
        print("üìö Showing sample books:")
        for book in response.objects:
            print(f"  - {book.properties['title']}")

"""We are using the @opik.track decorator and the OpenAI logging integration to automatically log our traces and spans. Learn more here https://www.comet.com/docs/opik/tracing/log_traces#using-an-integration"""

@opik.track
def retrieve_context(user_query):
    # Try different search methods
    try:
        # Try semantic/vector search first
        response = book_collection.query.near_text(
            query=user_query,
            limit=3
        )
    except Exception:
        try:
            # Fallback to BM25 search
            response = book_collection.query.bm25(
                query=user_query,
                limit=3
            )
        except Exception:
            # Final fallback: just get some books
            response = book_collection.query.fetch_objects(limit=3)

    recommended_books = []
    for book in response.objects:
        recommended_books.append(book.properties['title'])
    return recommended_books

@opik.track
def generate_response(user_query, recommended_books):
  prompt = f"""
  You're a helpful assistant, reply to a chatbot message for someone inquiring for
  book recommendations. The user query was {user_query}


  These were the book that were extracted from the vector
  search:

  {recommended_books}
  """

  messages=[
      {
          "role": "user",
          "content": prompt
      }
  ]
  response = call_llm(ollama_client, messages)


  return (response.choices[0].message.content)

@opik.track(name="rag-example")
def llm_chain(user_query):
    context = retrieve_context(user_query)
    response = generate_response(user_query, context)
    return response

# Use the LLM chain with test query
test_query = "fantasy books with magic"
print(f"\nüß† Running RAG chain for: {test_query}")
result = llm_chain(test_query)
print(f"\nü§ñ AI Response:\n{result}")

# Clean up the Weaviate connection
weaviate_client.close()
print("\n‚úÖ Weaviate connection closed properly")